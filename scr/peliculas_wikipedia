import requests
from bs4 import BeautifulSoup
import re
import time
import csv
import dateparser
import uuid 

WIKI_BASE = "https://es.wikipedia.org"

def get_soup(url):
    """Obtiene y parsea el HTML de una URL."""
    headers = {"User-Agent": "Mozilla/5.0"}
    resp = requests.get(url, headers=headers)
    resp.raise_for_status()
    return BeautifulSoup(resp.text, "html.parser")


def listar_peliculas_de_anio(subcat_url):
    """Lista todas las películas de la subcategoría, manejando paginación."""
    soup = get_soup(subcat_url)
    peliculas = []
    for ul in soup.select("div.mw-category div.mw-category-group ul"):
        for li in ul.find_all("li"):
            a = li.find("a")
            if a:
                peliculas.append((a.text.strip(), WIKI_BASE + a.get("href")))
    # manejar paginación
    next_link = soup.find("a", string="página siguiente")
    if next_link:
        next_url = WIKI_BASE + next_link.get("href")
        peliculas += listar_peliculas_de_anio(next_url)
    return peliculas


def extraer_trama(soup):
    """Extrae el primer párrafo debajo de 'Trama', 'Sinopsis' o 'Argumento'."""
    divs = soup.find_all("div", class_="mw-heading")
    for div in divs:
        h2 = div.find("h2")
        if h2 and h2.text.strip().lower() in ["trama", "sinopsis", "argumento"]:
            p = div.find_next_sibling("p")
            while p and p.name != "p":
                p = p.find_next_sibling()
            if p:
                return re.sub(r"\[\d+\]", "", p.get_text()).strip()
    # fallback: primer párrafo del contenido
    content_div = soup.find("div", class_="mw-parser-output")
    if content_div:
        for p in content_div.find_all("p", recursive=False):
            texto = p.get_text().strip()
            if texto:
                return texto
    return None


def parse_fecha(fecha_texto):
    """
    Convierte la fecha en formato 'dd/mm/yyyy' usando la fecha de España si existe.
    Si no se puede parsear, devuelve None.
    """
    if not fecha_texto or fecha_texto.strip() in ["?", ""]:
        return None

    fecha_texto = re.sub(r"\[\d+\]", "", fecha_texto)
    fecha_texto = re.sub(r"\s+", " ", fecha_texto).strip()

    # Buscar fechas con país
    matches = re.findall(r"(\d{1,2} de [A-Za-z]+ de \d{4})\s*\((.*?)\)", fecha_texto)
    fecha_elegida = None
    for fecha, pais in matches:
        if "espana" in pais.lower():
            fecha_elegida = fecha
            break
    if not fecha_elegida and matches:
        fecha_elegida = matches[0][0]

    # Parsear la fecha elegida
    if fecha_elegida:
        match = re.search(r"\d{1,2} de [A-Za-z]+ de \d{4}", fecha_elegida)
        if match:
            fecha_dt = dateparser.parse(match.group(0), languages=['es'], settings={'DATE_ORDER': 'DMY'})
            if fecha_dt:
                return fecha_dt.strftime("%d/%m/%Y")
            else:
                return None

    # fallback simple
    match = re.search(r"\d{1,2} de [A-Za-z]+ de \d{4}", fecha_texto)
    if match:
        fecha_dt = dateparser.parse(match.group(0), languages=['es'], settings={'DATE_ORDER': 'DMY'})
        if fecha_dt:
            return fecha_dt.strftime("%d/%m/%Y")

    return None


def extraer_info_pelicula(peli_url):
    """Extrae información de la película desde su página de Wikipedia."""
    soup = get_soup(peli_url)
    titulo_tag = soup.find("h1", id="firstHeading")
    titulo = re.sub(r"\s*\(.*?\)\s*", "", titulo_tag.text.strip()) if titulo_tag else None

    trama = extraer_trama(soup)

    fecha_estreno = None
    genero = None
    duracion = None
    infobox = soup.find("table", class_="infobox")
    if infobox:
        for tr in infobox.find_all("tr"):
            th = tr.find("th")
            td = tr.find("td")
            if th and td:
                key = th.get_text().strip().lower()
                valor = re.sub(r"\[\d+\]", "", td.get_text().strip())
                if "estreno" in key:
                    fecha_estreno = parse_fecha(valor.replace("\n", " "))
                elif "género" in key:
                    genero = valor.replace("\n", " ").strip() or None
                elif "duración" in key:
                    match = re.search(r"\d+", valor.replace("\n", " "))
                    duracion = int(match.group()) if match else None

    return titulo, fecha_estreno, genero, duracion, trama


base_url = f"{WIKI_BASE}/wiki/Categor%C3%ADa:Pel%C3%ADculas_de_Espa%C3%B1a_por_a%C3%B1o"
soup = get_soup(base_url)
subcats = []

for ul in soup.select("div.mw-category div.mw-category-group ul"):
    for li in ul.find_all("li"):
        a = li.find("a")
        if a and "Películas de España de" in a.text:
            year = int(a.text.strip()[-4:])
            if 1940 <= year <= 2025:  
                subcats.append((year, WIKI_BASE + a.get("href")))

subcats.sort(reverse=True)  # orden descendente

# Guardar en CSV
with open("peliculas_1940_2025_2.csv", "w", encoding="utf-8", newline="") as f:
    writer = csv.writer(f)
    # Cabecera FAIR / FACE (en inglés)
    writer.writerow(["id","title", "release_date", "year", "genre", "duration_minutes", "plot"])

    for year, url in subcats:
        print(f"Procesando año {year}...\n")
        peliculas = listar_peliculas_de_anio(url)

        for titulo_rel, peli_url in peliculas:
            if any(x in titulo_rel for x in ["Anexo", "Lista de"]):
                continue
            try:
                titulo, fecha, genero, duracion, trama = extraer_info_pelicula(peli_url)
                # Generar UUID único para cada película
                pelicula_id = str(uuid.uuid4())
                writer.writerow([pelicula_id, titulo, fecha, year, genero, duracion, trama])
            except Exception as e:
                print("Error en", peli_url, e)
            time.sleep(0.5)
